{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/py3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/anaconda3/envs/py3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.ensemble.bagging module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.ensemble. Anything that cannot be imported from sklearn.ensemble is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/anaconda3/envs/py3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.ensemble.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.ensemble. Anything that cannot be imported from sklearn.ensemble is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/anaconda3/envs/py3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.ensemble.forest module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.ensemble. Anything that cannot be imported from sklearn.ensemble is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "Using TensorFlow backend.\n",
      "/anaconda3/envs/py3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.utils.testing module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.utils. Anything that cannot be imported from sklearn.utils is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/anaconda3/envs/py3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.metrics.classification module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.metrics. Anything that cannot be imported from sklearn.metrics is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import math\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from models import normal_learning_model, normal_learning_model_last\n",
    "from models import generator_learning_model, classifier_learning_model\n",
    "import numpy as np;\n",
    "from models import utils_multidatasource, Optim\n",
    "import scipy\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io as sio\n",
    "from scipy.sparse.linalg import svds\n",
    "from sklearn.preprocessing import normalize\n",
    "from numpy import linalg as LA\n",
    "import pickle\n",
    "from torch.autograd import Variable\n",
    "from scipy.io import loadmat\n",
    "import torch.nn.functional as F\n",
    "from scipy import spatial\n",
    "from termcolor import colored\n",
    "from scipy.stats import sem\n",
    "import warnings\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from imblearn.metrics import geometric_mean_score\n",
    "\n",
    "# def extract(v):\n",
    "#     return v.data.storage().tolist()\n",
    "\n",
    "\n",
    "def train_normal(loader, data, model, criterion, optim, batch_size):\n",
    "    model.train();\n",
    "    total_loss = 0;\n",
    "    n_samples = 0;\n",
    "    for inputs in loader.get_batches(data, batch_size, True):        \n",
    "        X, Y, Labels = inputs[0], inputs[1], inputs[2]\n",
    "        model.zero_grad();\n",
    "        output = model(X);        \n",
    "        batch_loss = criterion(output, Y); \n",
    "        batch_loss.backward()\n",
    "        total_loss += batch_loss.data.item();\n",
    "        optim.step();\n",
    "        if len(output.shape)<4:\n",
    "            n_samples += (output.size(0) * output.size(1) * output.size(2));\n",
    "        else:\n",
    "            n_samples += (output.size(0) * output.size(1) * output.size(2) * output.size(3));\n",
    "    return total_loss / n_samples\n",
    "\n",
    "def test_normal(loader, data, model, criterion, optim, batch_size):\n",
    "    total_loss = 0;\n",
    "    n_samples = 0;\n",
    "    for inputs in loader.get_batches(data, batch_size, True):        \n",
    "        X, Y, Labels = inputs[0], inputs[1], inputs[2]\n",
    "        output = model(X);        \n",
    "        batch_loss = criterion(output, Y); \n",
    "        total_loss += batch_loss.data.item();\n",
    "        if len(output.shape)<4:\n",
    "            n_samples += (output.size(0) * output.size(1) * output.size(2));\n",
    "        else:\n",
    "            n_samples += (output.size(0) * output.size(1) * output.size(2) * output.size(3));\n",
    "    return total_loss / n_samples\n",
    "\n",
    "\n",
    "def train_classifier(loader, data, model, criterion, optim, batch_size, model_normal):\n",
    "    model.train();\n",
    "    total_loss = 0;\n",
    "    n_samples = 0;\n",
    "    Label_truth = torch.zeros((0,data[2].shape[1])) \n",
    "    Label_predict = torch.zeros((0,data[2].shape[1])) \n",
    "    for inputs in loader.get_batches(data, batch_size, True):        \n",
    "        X, Y, Labels = inputs[0], inputs[1], inputs[2]\n",
    "        model.zero_grad();\n",
    "        output = model(X, model_normal);             \n",
    "        Y_duplicate = Y.clone()\n",
    "        for class_idx in range(Labels.shape[1]-1):\n",
    "            Y_duplicate = torch.cat((Y_duplicate, Y),dim=0)\n",
    "        loss_all = criterion(output, Y_duplicate);     \n",
    "        Label_weighting = Labels[:,0]\n",
    "        for class_idx in range(Labels.shape[1]-1):\n",
    "            if torch.sum(Labels[:,class_idx+1])>0:\n",
    "                ratio = torch.sum(Labels[:,0])/torch.sum(Labels[:,class_idx+1])\n",
    "            else:\n",
    "                ratio = 1\n",
    "            Label_weighting = torch.cat((Label_weighting, ratio*Labels[:,class_idx+1]),dim=0) \n",
    "        Label_weighting.unsqueeze_(-1)\n",
    "        Label_weighting.unsqueeze_(-1)\n",
    "        Label_weighting.unsqueeze_(-1)\n",
    "        Label_weighting_new = Label_weighting.expand(len(Label_weighting),output.shape[1],output.shape[2],output.shape[3])     \n",
    "        batch_loss = torch.sum(torch.mul(loss_all, Label_weighting_new)) \n",
    "        graph_reg = 0\n",
    "        G_predict_cls, G_predict_cls_org = model_classifier.predict_relationship(model_normal)     \n",
    "        num_graph = len(G_predict_cls)\n",
    "        for graph_i in range(num_graph-1):\n",
    "            for graph_j in range(graph_i+1, num_graph):\n",
    "                graph_A = G_predict_cls[graph_i].reshape(Data.m*Data.m); \n",
    "                graph_B = G_predict_cls[graph_j].reshape(Data.m*Data.m);\n",
    "                graph_reg = graph_reg + 1/LA.norm(graph_A-graph_B)      \n",
    "        batch_loss = batch_loss + 0.01*graph_reg \n",
    "        batch_loss.backward()\n",
    "        total_loss += batch_loss.data.item();\n",
    "        optim.step();\n",
    "        if len(output.shape)<4:\n",
    "            n_samples += (output.size(0) * output.size(1) * output.size(2));\n",
    "        else:\n",
    "            n_samples += (output.size(0) * output.size(1) * output.size(2) * output.size(3));\n",
    "\n",
    "        #################### classification evaluation ##############\n",
    "        predict_label_tmp = torch.sum(loss_all, dim=1)\n",
    "        predict_label_tmp = torch.sum(predict_label_tmp, dim=1)\n",
    "        predict_label_tmp = torch.sum(predict_label_tmp, dim=1)\n",
    "        predict_label = predict_label_tmp.view(Labels.shape[1], Y.shape[0]).transpose(0,1)\n",
    "        predict_label = F.softmin(predict_label, dim =1)  \n",
    "        Label_predict = torch.cat((Label_predict, predict_label),dim=0)\n",
    "        Label_truth = torch.cat((Label_truth, Labels),dim=0)\n",
    "    \n",
    "    Label_predict = Label_predict.detach().numpy()\n",
    "    classification_report_trn = classification_report(np.argmax(Label_truth, axis=1), np.argmax(Label_predict, axis=1), output_dict=True)    \n",
    "    \n",
    "    return total_loss / n_samples, classification_report_trn\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_classifier(loader, data, model, criterion, optim, batch_size):\n",
    "    total_loss = 0;\n",
    "    n_samples = 0;\n",
    "    Label_truth = torch.zeros((0,data[2].shape[1])) \n",
    "    Label_predict = torch.zeros((0,model.num_class)) \n",
    "    for inputs in loader.get_batches(data, batch_size, True):        \n",
    "        X, Y, Labels = inputs[0], inputs[1], inputs[2]\n",
    "        output = model(X, model_normal);        \n",
    "        Y_duplicate = Y.clone()\n",
    "        for class_idx in range(model.num_class-1):\n",
    "            Y_duplicate = torch.cat((Y_duplicate, Y),dim=0)\n",
    "        loss_all = criterion(output, Y_duplicate); \n",
    "        total_loss += torch.sum(loss_all).data.item();\n",
    "        if len(output.shape)<4:\n",
    "            n_samples += (output.size(0) * output.size(1) * output.size(2));\n",
    "        else:\n",
    "            n_samples += (output.size(0) * output.size(1) * output.size(2) * output.size(3));\n",
    "                    \n",
    "        #################### classification evaluation ##############\n",
    "        predict_label_tmp = torch.sum(loss_all, dim=1)\n",
    "        predict_label_tmp = torch.sum(predict_label_tmp, dim=1)\n",
    "        predict_label_tmp = torch.sum(predict_label_tmp, dim=1)\n",
    "        predict_label = predict_label_tmp.view(model.num_class, X[0].shape[0]).transpose(0,1)\n",
    "        predict_label = F.softmin(predict_label, dim =1)  \n",
    "        Label_predict = torch.cat((Label_predict, predict_label),dim=0)\n",
    "        Label_truth = torch.cat((Label_truth, Labels),dim=0)\n",
    "    \n",
    "    Label_predict = Label_predict.detach().numpy()\n",
    "    Label_predict_sum = np.zeros(Label_truth.shape)\n",
    "    Label_predict_sum[:,0] = Label_predict[:,0]\n",
    "    #pdb.set_trace()\n",
    "    for class_i in range(Label_truth.shape[1]-1):\n",
    "        Label_predict_sum[:,class_i+1] = Label_predict[:,class_i+1] + Label_predict[:,class_i+1+Label_truth.shape[1]-1] \n",
    "        \n",
    "    Label_truth = np.argmax(Label_truth, axis=1)\n",
    "    Label_predict_sum = np.argmax(Label_predict_sum, axis=1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    classification_report_tst = classification_report(Label_truth, Label_predict_sum, output_dict=True) \n",
    "    \n",
    "    MF = (classification_report_tst['macro avg']['f1-score'])\n",
    "    GM = (geometric_mean_score(Label_truth, Label_predict_sum, average='macro'))\n",
    "    PC = (classification_report_tst['macro avg']['precision'])\n",
    "    AR = (accuracy_score(Label_truth, Label_predict_sum))\n",
    "    return total_loss / n_samples, classification_report_tst, MF, GM, PC, AR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class args:\n",
    "     \n",
    "    train = 0.9 ## \n",
    "    valid = 0.05##\n",
    "    \n",
    "    model_normal = 'normal_learning_model'\n",
    "    model_normal_last = 'normal_learning_model_last'\n",
    "    model_generator = 'generator_learning_model'\n",
    "    model_classifier = 'classifier_learning_model'\n",
    "       \n",
    "    window = 18           \n",
    "    pre_win = 3 \n",
    "    normal_win = 18\n",
    "    normal_prewin = 3\n",
    "    generator_win = 18    \n",
    "    generator_prewin = 1  \n",
    "    classifier_win = 15\n",
    "    classifier_prewin = 3\n",
    "\n",
    "    bootstrap_num = 3\n",
    "    epochs_simple = 50\n",
    "    epochs_last = 300\n",
    "    epochs_GANs = 10\n",
    "    rounds_LSTM = 10\n",
    "    syn_rounds = 1\n",
    "    y_dim = 65   \n",
    "\n",
    "    RUC_layers = 1\n",
    "    hidden_dim = 80 \n",
    "    reduce_dim = 60 \n",
    "    lowrank_minor = 3\n",
    "       \n",
    "    clip = 1.\n",
    "    lr_N = 0.01\n",
    "    lr_G = 0.01\n",
    "    lr_C = 0.01\n",
    "    lr_lstm = 0.01\n",
    "    \n",
    "    batch_size = 100\n",
    "    dropout = 0.001\n",
    "    gpu = None\n",
    "    cuda = False\n",
    "    optim = 'adam'#'adam'\n",
    "\n",
    "    weight_decay = 0\n",
    "    horizon = 1\n",
    "    output_fun = None\n",
    "    mask = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## initialize models\n",
    "warnings.filterwarnings('ignore')\n",
    "Data = utils_multidatasource.Data_utility(args)\n",
    "\n",
    "data_paths = ['10-20-30-edges/filter_norm_expression0.mat', '10-20-30-edges/filter_norm_expression1.mat', \n",
    "              '10-20-30-edges/filter_norm_expression2.mat', '10-20-30-edges/filter_norm_expression3.mat']\n",
    "graph_paths = ['10-20-30-edges/A0.mat', '10-20-30-edges/A1.mat', \n",
    "               '10-20-30-edges/A2.mat', '10-20-30-edges/A3.mat']\n",
    "traning_samples = [1000, 60, 60, 60]\n",
    "testing_samples = [1000, 2*traning_samples[1], 2*traning_samples[2], 2*traning_samples[3]]\n",
    "data_all = []\n",
    "G_groudtruth = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx_data in range(len(data_paths)):\n",
    "    data_tmp = sio.loadmat(data_paths[idx_data])['expression']\n",
    "    graph_tmp = sio.loadmat(graph_paths[idx_data])['A']\n",
    "    data_all.append(data_tmp)\n",
    "    G_groudtruth.append(graph_tmp)\n",
    "    \n",
    "criterion_1 = nn.MSELoss(size_average=False)\n",
    "criterion_2 = nn.MSELoss(size_average=False, reduce=False)\n",
    "criterion_3 = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "buliding model\n",
      "torch.Size([1000, 18, 65])\n",
      "torch.Size([1000, 18, 65])\n",
      "torch.Size([1060, 18, 65])\n",
      "torch.Size([1120, 18, 65])\n",
      "torch.Size([1120, 18, 65])\n",
      "torch.Size([1240, 18, 65])\n",
      "torch.Size([1180, 18, 65])\n",
      "torch.Size([1360, 18, 65])\n",
      "~~~~~~~~~~~~~~~~~~~ begin training/validating ~~~~~~~~~~~~~~~~\n"
     ]
    }
   ],
   "source": [
    "start_point = 0   \n",
    "Data.m = data_all[0].shape[1]\n",
    "print('buliding model')                \n",
    "###############################\n",
    "###### preparing dataset ######\n",
    "gap_point = 100\n",
    "X_trn_org = torch.zeros((0, args.window, Data.m)) ## input\n",
    "Y_trn_full_org = torch.zeros((0, args.window, args.pre_win, Data.m)) ### long target\n",
    "Label_trn_org = torch.zeros((np.sum(traning_samples), len(data_paths)))   ### label\n",
    "start_trn_idx = 0\n",
    "X_tst_org = torch.zeros((0, args.window, Data.m))\n",
    "Y_tst_full_org = torch.zeros((0, args.window, args.pre_win, Data.m))\n",
    "Label_tst_org = torch.zeros((np.sum(testing_samples), len(data_paths)))\n",
    "start_tst_idx = 0\n",
    "### concatenating data class by class \n",
    "for idx_data in range(len(data_paths)):\n",
    "            \n",
    "    data_tmp_train, data_tmp_valid, data_tmp_test =  Data._split(data_all[idx_data], args)\n",
    "            \n",
    "    X_trn_tmp      = data_tmp_train[0][start_point:(start_point+traning_samples[idx_data])]    \n",
    "    X_trn_org = torch.cat((X_trn_org, X_trn_tmp), dim = 0)\n",
    "    Y_trn_full_tmp = data_tmp_train[3][start_point:(start_point+traning_samples[idx_data])]  \n",
    "    Y_trn_full_org = torch.cat((Y_trn_full_org, Y_trn_full_tmp), dim = 0)\n",
    "    Label_trn_org[start_trn_idx:(start_trn_idx+traning_samples[idx_data]), idx_data] = 1   \n",
    "    start_trn_idx = start_trn_idx + traning_samples[idx_data]        \n",
    "    print(X_trn_org.shape)\n",
    "            \n",
    "    X_tst_tmp      = data_tmp_train[0][(start_point+traning_samples[idx_data]+gap_point):(start_point+traning_samples[idx_data]+gap_point+testing_samples[idx_data])] \n",
    "    Y_tst_full_tmp = data_tmp_train[3][(start_point+traning_samples[idx_data]+gap_point):(start_point+traning_samples[idx_data]+gap_point+testing_samples[idx_data])]     \n",
    "    X_tst_org = torch.cat((X_tst_org, X_tst_tmp), dim = 0)\n",
    "    Y_tst_full_org = torch.cat((Y_tst_full_org, Y_tst_full_tmp), dim = 0)\n",
    "    Label_tst_org[start_tst_idx:(start_tst_idx+testing_samples[idx_data]), idx_data] = 1   \n",
    "    start_tst_idx = start_tst_idx + testing_samples[idx_data]   \n",
    "    print(X_tst_org.shape)     \n",
    "        \n",
    "## copy of normal data (major data)\n",
    "trn_indices_normal = np.arange(traning_samples[0])\n",
    "np.random.shuffle(trn_indices_normal)\n",
    "X_trn_normal = X_trn_org[0:traning_samples[0]][trn_indices_normal] \n",
    "Y_trn_full_normal = Y_trn_full_org[0:traning_samples[0]][trn_indices_normal]\n",
    "Data_trn_full_normal = [X_trn_normal, Y_trn_full_normal, Label_trn_org[0:traning_samples[0]][trn_indices_normal]]\n",
    "\n",
    "## copy of event data (minor data)\n",
    "trn_indices_minor = np.arange(np.sum(traning_samples[1:]))\n",
    "np.random.shuffle(trn_indices_minor)\n",
    "X_trn_minor = X_trn_org[traning_samples[0]:][trn_indices_minor] \n",
    "Y_trn_full_minor = Y_trn_full_org[traning_samples[0]:][trn_indices_minor]\n",
    "Data_trn_full_minor = [X_trn_minor, Y_trn_full_minor, Label_trn_org[traning_samples[0]:][trn_indices_minor]]\n",
    "        \n",
    "#### prepraing training set (normal and event) for LSTM  ######\n",
    "Data_trn_full = [X_trn_org[:,0:args.classifier_win,:], Y_trn_full_org[:,0:args.classifier_win,:,:], Label_trn_org]\n",
    "#### preparing testing set #################\n",
    "#Data_tst_full is in the shape of (1300, 15, 65)\n",
    "Data_tst_full = [X_tst_org[:,0:args.classifier_win,:], Y_tst_full_org[:,0:args.classifier_win,:,:], Label_tst_org]\n",
    "        \n",
    "\n",
    "print(\"~~~~~~~~~~~~~~~~~~~ begin training/validating ~~~~~~~~~~~~~~~~\")          \n",
    "G_groundtruth_normal = G_groudtruth[0].reshape(Data.m*Data.m); \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bootstrap_idx  0|epoch   0|time:20.75s|tn_ls 1.314089|aupr 0.026139|\n",
      "bootstrap_idx  0|epoch   1|time:22.14s|tn_ls 1.001514|aupr 0.026744|\n",
      "bootstrap_idx  0|epoch   2|time:21.88s|tn_ls 0.985632|aupr 0.039676|\n",
      "bootstrap_idx  0|epoch   3|time:22.26s|tn_ls 0.964525|aupr 0.082864|\n",
      "bootstrap_idx  0|epoch   4|time:23.30s|tn_ls 0.932782|aupr 0.136940|\n",
      "bootstrap_idx  0|epoch   5|time:21.34s|tn_ls 0.895609|aupr 0.165290|\n",
      "bootstrap_idx  0|epoch   6|time:21.45s|tn_ls 0.856999|aupr 0.188987|\n",
      "bootstrap_idx  0|epoch   7|time:24.28s|tn_ls 0.826074|aupr 0.210035|\n",
      "bootstrap_idx  0|epoch   8|time:8165.67s|tn_ls 0.803406|aupr 0.234614|\n",
      "bootstrap_idx  0|epoch   9|time:23.97s|tn_ls 0.785142|aupr 0.255033|\n",
      "bootstrap_idx  0|epoch  10|time:21.58s|tn_ls 0.769267|aupr 0.276013|\n",
      "bootstrap_idx  0|epoch  11|time:26.46s|tn_ls 0.755983|aupr 0.289760|\n",
      "bootstrap_idx  0|epoch  12|time:22.10s|tn_ls 0.744721|aupr 0.300089|\n",
      "bootstrap_idx  0|epoch  13|time:22.47s|tn_ls 0.734312|aupr 0.312327|\n",
      "bootstrap_idx  0|epoch  14|time:20.88s|tn_ls 0.725665|aupr 0.327732|\n",
      "bootstrap_idx  0|epoch  15|time:23.06s|tn_ls 0.716123|aupr 0.337930|\n",
      "bootstrap_idx  0|epoch  16|time:22.22s|tn_ls 0.706222|aupr 0.352391|\n",
      "bootstrap_idx  0|epoch  17|time:21.96s|tn_ls 0.695591|aupr 0.368481|\n",
      "bootstrap_idx  0|epoch  18|time:23.91s|tn_ls 0.683819|aupr 0.384052|\n",
      "bootstrap_idx  0|epoch  19|time:23.31s|tn_ls 0.673335|aupr 0.400117|\n",
      "bootstrap_idx  0|epoch  20|time:22.36s|tn_ls 0.660032|aupr 0.418773|\n",
      "bootstrap_idx  0|epoch  21|time:22.23s|tn_ls 0.647339|aupr 0.438138|\n",
      "bootstrap_idx  0|epoch  22|time:21.17s|tn_ls 0.633086|aupr 0.456564|\n",
      "bootstrap_idx  0|epoch  23|time:24.33s|tn_ls 0.617533|aupr 0.479040|\n",
      "bootstrap_idx  0|epoch  24|time:21.81s|tn_ls 0.601730|aupr 0.498703|\n",
      "bootstrap_idx  0|epoch  25|time:21.41s|tn_ls 0.586725|aupr 0.517004|\n",
      "bootstrap_idx  0|epoch  26|time:20.82s|tn_ls 0.572294|aupr 0.531951|\n",
      "bootstrap_idx  0|epoch  27|time:19.49s|tn_ls 0.559032|aupr 0.547773|\n",
      "bootstrap_idx  0|epoch  28|time:20.35s|tn_ls 0.546774|aupr 0.560794|\n",
      "bootstrap_idx  0|epoch  29|time:20.39s|tn_ls 0.535310|aupr 0.574924|\n",
      "bootstrap_idx  0|epoch  30|time:20.74s|tn_ls 0.524006|aupr 0.589557|\n",
      "bootstrap_idx  0|epoch  31|time:20.92s|tn_ls 0.513045|aupr 0.602158|\n",
      "bootstrap_idx  0|epoch  32|time:19.98s|tn_ls 0.502625|aupr 0.615280|\n",
      "bootstrap_idx  0|epoch  33|time:20.21s|tn_ls 0.493249|aupr 0.627996|\n",
      "bootstrap_idx  0|epoch  34|time:19.41s|tn_ls 0.484887|aupr 0.637754|\n",
      "bootstrap_idx  0|epoch  35|time:19.21s|tn_ls 0.476968|aupr 0.644571|\n",
      "bootstrap_idx  0|epoch  36|time:19.88s|tn_ls 0.470330|aupr 0.649281|\n",
      "bootstrap_idx  0|epoch  37|time:19.73s|tn_ls 0.463443|aupr 0.654771|\n",
      "bootstrap_idx  0|epoch  38|time:19.20s|tn_ls 0.457131|aupr 0.660112|\n",
      "bootstrap_idx  0|epoch  39|time:19.83s|tn_ls 0.450800|aupr 0.662292|\n",
      "bootstrap_idx  0|epoch  40|time:19.89s|tn_ls 0.445068|aupr 0.664684|\n",
      "bootstrap_idx  0|epoch  41|time:19.23s|tn_ls 0.439289|aupr 0.667104|\n",
      "bootstrap_idx  0|epoch  42|time:18.86s|tn_ls 0.435053|aupr 0.669282|\n",
      "bootstrap_idx  0|epoch  43|time:18.34s|tn_ls 0.430306|aupr 0.671579|\n",
      "bootstrap_idx  0|epoch  44|time:18.38s|tn_ls 0.426341|aupr 0.670967|\n",
      "bootstrap_idx  0|epoch  45|time:19.18s|tn_ls 0.422275|aupr 0.670343|\n",
      "bootstrap_idx  0|epoch  46|time:20.90s|tn_ls 0.418044|aupr 0.670396|\n",
      "bootstrap_idx  0|epoch  47|time:20.00s|tn_ls 0.414610|aupr 0.670604|\n",
      "bootstrap_idx  0|epoch  48|time:19.32s|tn_ls 0.411073|aupr 0.670480|\n",
      "bootstrap_idx  0|epoch  49|time:19.94s|tn_ls 0.407449|aupr 0.670035|\n",
      "bootstrap_idx  1|epoch   0|time:20.16s|tn_ls 1.290219|aupr 0.024776|\n",
      "bootstrap_idx  1|epoch   1|time:19.93s|tn_ls 0.998379|aupr 0.029957|\n",
      "bootstrap_idx  1|epoch   2|time:21.49s|tn_ls 0.985255|aupr 0.054050|\n",
      "bootstrap_idx  1|epoch   3|time:20.97s|tn_ls 0.965896|aupr 0.107648|\n",
      "bootstrap_idx  1|epoch   4|time:19.59s|tn_ls 0.931021|aupr 0.190011|\n",
      "bootstrap_idx  1|epoch   5|time:19.83s|tn_ls 0.891182|aupr 0.286276|\n",
      "bootstrap_idx  1|epoch   6|time:18.06s|tn_ls 0.852485|aupr 0.356647|\n",
      "bootstrap_idx  1|epoch   7|time:19.92s|tn_ls 0.819488|aupr 0.411066|\n",
      "bootstrap_idx  1|epoch   8|time:20.41s|tn_ls 0.793221|aupr 0.437822|\n",
      "bootstrap_idx  1|epoch   9|time:19.06s|tn_ls 0.774712|aupr 0.453682|\n",
      "bootstrap_idx  1|epoch  10|time:18.99s|tn_ls 0.759197|aupr 0.463984|\n",
      "bootstrap_idx  1|epoch  11|time:19.15s|tn_ls 0.745855|aupr 0.469164|\n",
      "bootstrap_idx  1|epoch  12|time:19.78s|tn_ls 0.733901|aupr 0.473271|\n",
      "bootstrap_idx  1|epoch  13|time:19.81s|tn_ls 0.723524|aupr 0.476322|\n",
      "bootstrap_idx  1|epoch  14|time:19.67s|tn_ls 0.714025|aupr 0.480274|\n",
      "bootstrap_idx  1|epoch  15|time:19.69s|tn_ls 0.705632|aupr 0.491790|\n",
      "bootstrap_idx  1|epoch  16|time:20.41s|tn_ls 0.696455|aupr 0.498745|\n",
      "bootstrap_idx  1|epoch  17|time:20.61s|tn_ls 0.686215|aupr 0.510716|\n",
      "bootstrap_idx  1|epoch  18|time:20.69s|tn_ls 0.674515|aupr 0.523755|\n",
      "bootstrap_idx  1|epoch  19|time:22.22s|tn_ls 0.661542|aupr 0.540152|\n",
      "bootstrap_idx  1|epoch  20|time:21.22s|tn_ls 0.647642|aupr 0.558591|\n",
      "bootstrap_idx  1|epoch  21|time:19.17s|tn_ls 0.632233|aupr 0.577477|\n",
      "bootstrap_idx  1|epoch  22|time:20.72s|tn_ls 0.617286|aupr 0.591341|\n",
      "bootstrap_idx  1|epoch  23|time:18.75s|tn_ls 0.601265|aupr 0.607296|\n",
      "bootstrap_idx  1|epoch  24|time:19.51s|tn_ls 0.585317|aupr 0.622002|\n",
      "bootstrap_idx  1|epoch  25|time:19.01s|tn_ls 0.570243|aupr 0.634834|\n",
      "bootstrap_idx  1|epoch  26|time:20.34s|tn_ls 0.557597|aupr 0.647291|\n",
      "bootstrap_idx  1|epoch  27|time:19.68s|tn_ls 0.548428|aupr 0.656020|\n",
      "bootstrap_idx  1|epoch  28|time:20.30s|tn_ls 0.539039|aupr 0.664201|\n",
      "bootstrap_idx  1|epoch  29|time:20.58s|tn_ls 0.531185|aupr 0.671453|\n",
      "bootstrap_idx  1|epoch  30|time:19.69s|tn_ls 0.523228|aupr 0.678089|\n",
      "bootstrap_idx  1|epoch  31|time:20.54s|tn_ls 0.516041|aupr 0.685552|\n",
      "bootstrap_idx  1|epoch  32|time:20.21s|tn_ls 0.508810|aupr 0.691600|\n",
      "bootstrap_idx  1|epoch  33|time:20.14s|tn_ls 0.502732|aupr 0.696447|\n",
      "bootstrap_idx  1|epoch  34|time:19.75s|tn_ls 0.496384|aupr 0.699920|\n",
      "bootstrap_idx  1|epoch  35|time:19.50s|tn_ls 0.491382|aupr 0.704313|\n",
      "bootstrap_idx  1|epoch  36|time:20.60s|tn_ls 0.486182|aupr 0.708896|\n",
      "bootstrap_idx  1|epoch  37|time:19.56s|tn_ls 0.482469|aupr 0.712371|\n",
      "bootstrap_idx  1|epoch  38|time:20.59s|tn_ls 0.477583|aupr 0.715246|\n",
      "bootstrap_idx  1|epoch  39|time:20.12s|tn_ls 0.473129|aupr 0.717970|\n",
      "bootstrap_idx  1|epoch  40|time:20.07s|tn_ls 0.467799|aupr 0.720871|\n",
      "bootstrap_idx  1|epoch  41|time:21.76s|tn_ls 0.463499|aupr 0.725617|\n",
      "bootstrap_idx  1|epoch  42|time:20.28s|tn_ls 0.459161|aupr 0.729784|\n",
      "bootstrap_idx  1|epoch  43|time:19.97s|tn_ls 0.453955|aupr 0.733215|\n",
      "bootstrap_idx  1|epoch  44|time:20.81s|tn_ls 0.449505|aupr 0.735503|\n",
      "bootstrap_idx  1|epoch  45|time:20.22s|tn_ls 0.445115|aupr 0.737522|\n",
      "bootstrap_idx  1|epoch  46|time:18.77s|tn_ls 0.440810|aupr 0.740018|\n",
      "bootstrap_idx  1|epoch  47|time:18.75s|tn_ls 0.436754|aupr 0.741009|\n",
      "bootstrap_idx  1|epoch  48|time:19.60s|tn_ls 0.432137|aupr 0.742639|\n",
      "bootstrap_idx  1|epoch  49|time:21.36s|tn_ls 0.428088|aupr 0.743153|\n",
      "bootstrap_idx  2|epoch   0|time:19.76s|tn_ls 1.254987|aupr 0.026643|\n",
      "bootstrap_idx  2|epoch   1|time:20.32s|tn_ls 0.999089|aupr 0.030985|\n",
      "bootstrap_idx  2|epoch   2|time:18.26s|tn_ls 0.988308|aupr 0.057061|\n",
      "bootstrap_idx  2|epoch   3|time:18.11s|tn_ls 0.973881|aupr 0.116413|\n",
      "bootstrap_idx  2|epoch   4|time:17.96s|tn_ls 0.940503|aupr 0.196429|\n",
      "bootstrap_idx  2|epoch   5|time:21.51s|tn_ls 0.890817|aupr 0.252836|\n",
      "bootstrap_idx  2|epoch   6|time:18.65s|tn_ls 0.842173|aupr 0.303076|\n",
      "bootstrap_idx  2|epoch   7|time:19.05s|tn_ls 0.808183|aupr 0.328978|\n",
      "bootstrap_idx  2|epoch   8|time:18.05s|tn_ls 0.783962|aupr 0.337831|\n",
      "bootstrap_idx  2|epoch   9|time:18.10s|tn_ls 0.764141|aupr 0.349982|\n",
      "bootstrap_idx  2|epoch  10|time:20.42s|tn_ls 0.747840|aupr 0.366006|\n",
      "bootstrap_idx  2|epoch  11|time:18.83s|tn_ls 0.733511|aupr 0.379634|\n",
      "bootstrap_idx  2|epoch  12|time:18.68s|tn_ls 0.720050|aupr 0.393168|\n",
      "bootstrap_idx  2|epoch  13|time:19.20s|tn_ls 0.707986|aupr 0.403406|\n",
      "bootstrap_idx  2|epoch  14|time:20.88s|tn_ls 0.694851|aupr 0.418054|\n",
      "bootstrap_idx  2|epoch  15|time:18.71s|tn_ls 0.679725|aupr 0.440752|\n",
      "bootstrap_idx  2|epoch  16|time:21.07s|tn_ls 0.662097|aupr 0.469080|\n",
      "bootstrap_idx  2|epoch  17|time:19.50s|tn_ls 0.642753|aupr 0.503302|\n",
      "bootstrap_idx  2|epoch  18|time:19.30s|tn_ls 0.623892|aupr 0.539299|\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bootstrap_idx  2|epoch  19|time:18.64s|tn_ls 0.605345|aupr 0.571974|\n",
      "bootstrap_idx  2|epoch  20|time:19.56s|tn_ls 0.587689|aupr 0.598240|\n",
      "bootstrap_idx  2|epoch  21|time:18.82s|tn_ls 0.571220|aupr 0.620854|\n",
      "bootstrap_idx  2|epoch  22|time:20.07s|tn_ls 0.556033|aupr 0.638390|\n",
      "bootstrap_idx  2|epoch  23|time:19.59s|tn_ls 0.542808|aupr 0.654588|\n",
      "bootstrap_idx  2|epoch  24|time:18.77s|tn_ls 0.530461|aupr 0.667749|\n",
      "bootstrap_idx  2|epoch  25|time:21.63s|tn_ls 0.519061|aupr 0.680130|\n",
      "bootstrap_idx  2|epoch  26|time:18.91s|tn_ls 0.508932|aupr 0.688445|\n",
      "bootstrap_idx  2|epoch  27|time:20.02s|tn_ls 0.499049|aupr 0.693584|\n",
      "bootstrap_idx  2|epoch  28|time:18.52s|tn_ls 0.491118|aupr 0.701521|\n",
      "bootstrap_idx  2|epoch  29|time:19.31s|tn_ls 0.483271|aupr 0.710105|\n",
      "bootstrap_idx  2|epoch  30|time:19.80s|tn_ls 0.475743|aupr 0.716441|\n",
      "bootstrap_idx  2|epoch  31|time:19.48s|tn_ls 0.468763|aupr 0.722135|\n",
      "bootstrap_idx  2|epoch  32|time:18.95s|tn_ls 0.462189|aupr 0.729323|\n",
      "bootstrap_idx  2|epoch  33|time:20.00s|tn_ls 0.456606|aupr 0.734353|\n",
      "bootstrap_idx  2|epoch  34|time:18.99s|tn_ls 0.450916|aupr 0.739444|\n",
      "bootstrap_idx  2|epoch  35|time:19.34s|tn_ls 0.445141|aupr 0.742640|\n",
      "bootstrap_idx  2|epoch  36|time:19.49s|tn_ls 0.439891|aupr 0.745858|\n",
      "bootstrap_idx  2|epoch  37|time:21.19s|tn_ls 0.435002|aupr 0.748469|\n",
      "bootstrap_idx  2|epoch  38|time:22.50s|tn_ls 0.430195|aupr 0.749779|\n",
      "bootstrap_idx  2|epoch  39|time:20.47s|tn_ls 0.425790|aupr 0.751789|\n",
      "bootstrap_idx  2|epoch  40|time:19.07s|tn_ls 0.422008|aupr 0.752742|\n",
      "bootstrap_idx  2|epoch  41|time:18.61s|tn_ls 0.417573|aupr 0.753033|\n",
      "bootstrap_idx  2|epoch  42|time:18.93s|tn_ls 0.414579|aupr 0.752532|\n",
      "bootstrap_idx  2|epoch  43|time:18.99s|tn_ls 0.411164|aupr 0.752472|\n",
      "bootstrap_idx  2|epoch  44|time:18.69s|tn_ls 0.407905|aupr 0.753060|\n",
      "bootstrap_idx  2|epoch  45|time:19.05s|tn_ls 0.404572|aupr 0.753053|\n",
      "bootstrap_idx  2|epoch  46|time:19.00s|tn_ls 0.401431|aupr 0.753036|\n",
      "bootstrap_idx  2|epoch  47|time:19.02s|tn_ls 0.398558|aupr 0.752449|\n",
      "bootstrap_idx  2|epoch  48|time:19.56s|tn_ls 0.395475|aupr 0.750998|\n",
      "bootstrap_idx  2|epoch  49|time:19.19s|tn_ls 0.392324|aupr 0.750033|\n",
      "aggregate aupr: 0.8097102725270611\n"
     ]
    }
   ],
   "source": [
    "G_predict_aggregate = np.zeros((Data.m, Data.m))\n",
    "for bootstrap_idx in range(args.bootstrap_num):\n",
    "    ## initializing models\n",
    "    args.num_class = 1\n",
    "    model_normal = eval(args.model_normal).Model(args, Data);\n",
    "    optim_normal = Optim.Optim(\n",
    "        model_normal.parameters(), args.optim, args.lr_N, args.clip, weight_decay = args.weight_decay,\n",
    "    )\n",
    "\n",
    "    trn_indices_normal = np.arange(900)\n",
    "    np.random.shuffle(trn_indices_normal)\n",
    "    trn_indices_normal = trn_indices_normal[0:800]\n",
    "    X_trn_normal_spl = X_trn_org[0:traning_samples[0]][trn_indices_normal] \n",
    "    Y_trn_full_normal_spl = Y_trn_full_org[0:traning_samples[0]][trn_indices_normal]\n",
    "    Data_trn_full_normal_spl = [X_trn_normal_spl, Y_trn_full_normal_spl, Label_trn_org[0:traning_samples[0]][trn_indices_normal]]\n",
    "\n",
    "     \n",
    "    for epoch in range(0, args.epochs_simple): \n",
    "        round_start_time = time.time()\n",
    "        train_normal_loss = train_normal(Data, Data_trn_full_normal_spl, model_normal, criterion_1, optim_normal, args.batch_size)    \n",
    "        G_predict, G_predict_org = model_normal.predict_relationship()\n",
    "\n",
    "        G_predict_normal = G_predict[0].reshape(Data.m*Data.m);\n",
    "        precision, recall, thresholds = metrics.precision_recall_curve(G_groundtruth_normal, G_predict_normal)\n",
    "        aupr = metrics.auc(recall, precision)\n",
    "        print('bootstrap_idx{:3d}|epoch {:3d}|time:{:5.2f}s|tn_ls {:5.6f}|aupr {:5.6f}|'.format(bootstrap_idx, epoch, (time.time() - round_start_time), train_normal_loss, aupr)) \n",
    "\n",
    "    G_predict_aggregate = G_predict_aggregate + G_predict[0]\n",
    "\n",
    "G_predict_aggregate_normal = (G_predict_aggregate/args.bootstrap_num);\n",
    "precision, recall, thresholds = metrics.precision_recall_curve(G_groundtruth_normal, G_predict_aggregate_normal.reshape(Data.m*Data.m))\n",
    "aupr = metrics.auc(recall, precision)\n",
    "print('aggregate aupr:', aupr) \n",
    "\n",
    "pickle.dump( G_predict_aggregate_normal, open( \"G_predict_aggregate_normal_A60_0121.pkl\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal last|epoch  0|time:21.77s|tn_ls 1.221932|vl_ls 0.990601\n",
      "Normal last|epoch  1|time:21.01s|tn_ls 0.991298|vl_ls 0.976201\n",
      "Normal last|epoch  2|time:20.78s|tn_ls 0.968938|vl_ls 0.942024\n",
      "Normal last|epoch  3|time:21.61s|tn_ls 0.931368|vl_ls 0.918543\n",
      "Normal last|epoch  4|time:21.97s|tn_ls 0.907393|vl_ls 0.897918\n",
      "Normal last|epoch  5|time:24.60s|tn_ls 0.871677|vl_ls 0.859395\n",
      "Normal last|epoch  6|time:23.73s|tn_ls 0.841219|vl_ls 0.833444\n",
      "Normal last|epoch  7|time:24.88s|tn_ls 0.815676|vl_ls 0.804554\n",
      "Normal last|epoch  8|time:23.19s|tn_ls 0.790033|vl_ls 0.781495\n",
      "Normal last|epoch  9|time:23.10s|tn_ls 0.769550|vl_ls 0.766734\n",
      "Normal last|epoch 10|time:23.92s|tn_ls 0.751053|vl_ls 0.750105\n",
      "Normal last|epoch 11|time:24.26s|tn_ls 0.735547|vl_ls 0.731780\n",
      "Normal last|epoch 12|time:22.97s|tn_ls 0.717362|vl_ls 0.715323\n",
      "Normal last|epoch 13|time:23.80s|tn_ls 0.699317|vl_ls 0.701053\n",
      "Normal last|epoch 14|time:24.39s|tn_ls 0.681273|vl_ls 0.685043\n",
      "Normal last|epoch 15|time:25.19s|tn_ls 0.667670|vl_ls 0.675557\n",
      "Normal last|epoch 16|time:25.22s|tn_ls 0.653503|vl_ls 0.668246\n",
      "Normal last|epoch 17|time:23.02s|tn_ls 0.641473|vl_ls 0.655935\n",
      "Normal last|epoch 18|time:23.95s|tn_ls 0.629202|vl_ls 0.644335\n",
      "Normal last|epoch 19|time:22.50s|tn_ls 0.622503|vl_ls 0.640213\n",
      "Normal last|epoch 20|time:24.42s|tn_ls 0.618278|vl_ls 0.632817\n",
      "Normal last|epoch 21|time:22.58s|tn_ls 0.604306|vl_ls 0.623988\n",
      "Normal last|epoch 22|time:24.90s|tn_ls 0.597868|vl_ls 0.617025\n",
      "Normal last|epoch 23|time:28.24s|tn_ls 0.592051|vl_ls 0.614038\n",
      "Normal last|epoch 24|time:25.07s|tn_ls 0.583931|vl_ls 0.608323\n",
      "Normal last|epoch 25|time:24.12s|tn_ls 0.577446|vl_ls 0.609839\n",
      "Normal last|epoch 26|time:23.13s|tn_ls 0.571226|vl_ls 0.600538\n",
      "Normal last|epoch 27|time:23.42s|tn_ls 0.565773|vl_ls 0.595944\n",
      "Normal last|epoch 28|time:23.34s|tn_ls 0.560334|vl_ls 0.599365\n",
      "Normal last|epoch 29|time:24.42s|tn_ls 0.561334|vl_ls 0.597560\n",
      "Normal last|epoch 30|time:23.37s|tn_ls 0.557723|vl_ls 0.595335\n",
      "Normal last|epoch 31|time:23.42s|tn_ls 0.554655|vl_ls 0.590641\n",
      "Normal last|epoch 32|time:24.55s|tn_ls 0.550345|vl_ls 0.588502\n",
      "Normal last|epoch 33|time:23.57s|tn_ls 0.547338|vl_ls 0.584639\n",
      "Normal last|epoch 34|time:23.01s|tn_ls 0.543677|vl_ls 0.583286\n",
      "Normal last|epoch 35|time:23.34s|tn_ls 0.540360|vl_ls 0.585772\n",
      "Normal last|epoch 36|time:24.22s|tn_ls 0.538094|vl_ls 0.578920\n",
      "Normal last|epoch 37|time:24.18s|tn_ls 0.534849|vl_ls 0.575831\n",
      "Normal last|epoch 38|time:24.91s|tn_ls 0.532830|vl_ls 0.576479\n",
      "Normal last|epoch 39|time:24.38s|tn_ls 0.530104|vl_ls 0.573684\n",
      "Normal last|epoch 40|time:24.41s|tn_ls 0.527710|vl_ls 0.573027\n",
      "Normal last|epoch 41|time:22.91s|tn_ls 0.527051|vl_ls 0.572976\n",
      "Normal last|epoch 42|time:23.55s|tn_ls 0.522086|vl_ls 0.574577\n",
      "Normal last|epoch 43|time:23.08s|tn_ls 0.517433|vl_ls 0.562300\n",
      "Normal last|epoch 44|time:24.84s|tn_ls 0.519044|vl_ls 0.570246\n",
      "Normal last|epoch 45|time:23.61s|tn_ls 0.513762|vl_ls 0.564613\n",
      "Normal last|epoch 46|time:24.43s|tn_ls 0.511237|vl_ls 0.565853\n",
      "Normal last|epoch 47|time:24.45s|tn_ls 0.509559|vl_ls 0.560588\n",
      "Normal last|epoch 48|time:25.40s|tn_ls 0.506355|vl_ls 0.561821\n",
      "Normal last|epoch 49|time:23.62s|tn_ls 0.504274|vl_ls 0.566020\n",
      "Normal last|epoch 50|time:24.11s|tn_ls 0.502782|vl_ls 0.555173\n",
      "Normal last|epoch 51|time:23.24s|tn_ls 0.501192|vl_ls 0.558148\n",
      "Normal last|epoch 52|time:23.69s|tn_ls 0.497890|vl_ls 0.552705\n",
      "Normal last|epoch 53|time:24.53s|tn_ls 0.496517|vl_ls 0.557789\n",
      "Normal last|epoch 54|time:22.08s|tn_ls 0.494928|vl_ls 0.556243\n",
      "Normal last|epoch 55|time:23.52s|tn_ls 0.492255|vl_ls 0.551641\n",
      "Normal last|epoch 56|time:24.31s|tn_ls 0.490842|vl_ls 0.547786\n",
      "Normal last|epoch 57|time:24.08s|tn_ls 0.488153|vl_ls 0.550299\n",
      "Normal last|epoch 58|time:25.13s|tn_ls 0.486334|vl_ls 0.547148\n",
      "Normal last|epoch 59|time:22.23s|tn_ls 0.483877|vl_ls 0.542782\n",
      "Normal last|epoch 60|time:23.85s|tn_ls 0.485457|vl_ls 0.549708\n",
      "Normal last|epoch 61|time:22.96s|tn_ls 0.483691|vl_ls 0.546610\n",
      "Normal last|epoch 62|time:25.05s|tn_ls 0.481933|vl_ls 0.542800\n",
      "Normal last|epoch 63|time:22.15s|tn_ls 0.478999|vl_ls 0.546848\n",
      "Normal last|epoch 64|time:22.58s|tn_ls 0.477230|vl_ls 0.545607\n",
      "Normal last|epoch 65|time:21.09s|tn_ls 0.475266|vl_ls 0.542005\n",
      "Normal last|epoch 66|time:22.52s|tn_ls 0.473340|vl_ls 0.539410\n",
      "Normal last|epoch 67|time:22.03s|tn_ls 0.471466|vl_ls 0.547917\n",
      "Normal last|epoch 68|time:22.58s|tn_ls 0.469864|vl_ls 0.537131\n",
      "Normal last|epoch 69|time:21.24s|tn_ls 0.470966|vl_ls 0.540749\n",
      "Normal last|epoch 70|time:22.25s|tn_ls 0.471075|vl_ls 0.537496\n",
      "Normal last|epoch 71|time:21.24s|tn_ls 0.465834|vl_ls 0.538939\n",
      "Normal last|epoch 72|time:21.86s|tn_ls 0.461975|vl_ls 0.535895\n",
      "Normal last|epoch 73|time:22.24s|tn_ls 0.463527|vl_ls 0.542677\n",
      "Normal last|epoch 74|time:20.78s|tn_ls 0.462536|vl_ls 0.536167\n",
      "Normal last|epoch 75|time:21.94s|tn_ls 0.460449|vl_ls 0.535324\n",
      "Normal last|epoch 76|time:23.13s|tn_ls 0.460195|vl_ls 0.534564\n",
      "Normal last|epoch 77|time:23.74s|tn_ls 0.455899|vl_ls 0.532243\n",
      "Normal last|epoch 78|time:22.56s|tn_ls 0.456551|vl_ls 0.534989\n",
      "Normal last|epoch 79|time:23.18s|tn_ls 0.455012|vl_ls 0.538227\n",
      "Normal last|epoch 80|time:22.61s|tn_ls 0.452751|vl_ls 0.550175\n",
      "Normal last|epoch 81|time:23.89s|tn_ls 0.455825|vl_ls 0.533387\n",
      "Normal last|epoch 82|time:22.49s|tn_ls 0.450485|vl_ls 0.539689\n",
      "Normal last|epoch 83|time:23.02s|tn_ls 0.448721|vl_ls 0.533006\n",
      "Normal last|epoch 84|time:22.62s|tn_ls 0.446719|vl_ls 0.533149\n",
      "Normal last|epoch 85|time:25.12s|tn_ls 0.446316|vl_ls 0.537112\n",
      "Normal last|epoch 86|time:22.96s|tn_ls 0.445977|vl_ls 0.532425\n",
      "Normal last|epoch 87|time:25.94s|tn_ls 0.447598|vl_ls 0.540078\n",
      "Normal last|epoch 88|time:23.82s|tn_ls 0.445049|vl_ls 0.532562\n"
     ]
    }
   ],
   "source": [
    "major_graph = torch.from_numpy(G_predict_aggregate_normal).float()\n",
    "args.num_class = 1\n",
    "model_normal = eval(args.model_normal_last).Model(args, Data, major_graph);\n",
    "optim_normal = Optim.Optim(\n",
    "    model_normal.parameters(), args.optim, args.lr_N, args.clip, weight_decay = args.weight_decay,\n",
    ")\n",
    "\n",
    "trn_num = 900\n",
    "indices_normal = np.arange(traning_samples[0])\n",
    "trn_indices_normal = indices_normal[0:trn_num]\n",
    "val_indices_normal = indices_normal[trn_num:]\n",
    "\n",
    "X_trn_normal_spl = X_trn_org[0:traning_samples[0]][trn_indices_normal] \n",
    "Y_trn_full_normal_spl = Y_trn_full_org[0:traning_samples[0]][trn_indices_normal]\n",
    "Data_trn_full_normal_spl = [X_trn_normal_spl, Y_trn_full_normal_spl, Label_trn_org[0:traning_samples[0]][trn_indices_normal]]\n",
    "\n",
    "X_val_normal_spl = X_trn_org[0:traning_samples[0]][val_indices_normal] \n",
    "Y_val_full_normal_spl = Y_trn_full_org[0:traning_samples[0]][val_indices_normal]\n",
    "Data_val_full_normal_spl = [X_val_normal_spl, Y_val_full_normal_spl, Label_trn_org[0:traning_samples[0]][val_indices_normal]]\n",
    "\n",
    "\n",
    "for epoch in range(0, args.epochs_last): \n",
    "    epoch_start_time = time.time() \n",
    "    train_normal_loss = train_normal(Data, Data_trn_full_normal_spl, model_normal, criterion_1, optim_normal, args.batch_size)       \n",
    "    val_normal_loss = test_normal(Data, Data_val_full_normal_spl, model_normal, criterion_1, optim_normal, args.batch_size)    \n",
    "    print('Normal last|epoch{:3d}|time:{:5.2f}s|tn_ls {:5.6f}|vl_ls {:5.6f}'.format(epoch, (time.time() - epoch_start_time), train_normal_loss, val_normal_loss)) \n",
    "\n",
    "pickle.dump( model_normal, open( \"model_normal_A60_0121.pkl\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gen_num = traning_samples[1]\n",
    "########################################################\n",
    "########## initializing generator/classifier ###########\n",
    "########################################################\n",
    "args.num_class = len(data_paths)-1    ###### generator of GAN ######   \n",
    "model_generator = eval(args.model_generator).Model(args, Data, model_normal);\n",
    "optim_generator = Optim.Optim(\n",
    "    model_generator.parameters(), args.optim, args.lr_G, args.clip, weight_decay = args.weight_decay,\n",
    ")  \n",
    "        \n",
    "args.num_class = len(data_paths) + len(data_paths)-1  ###### classifier of GAN ######  \n",
    "model_classifier = eval(args.model_classifier).Model(args, Data, model_normal);         \n",
    "optim_classifier = Optim.Optim(\n",
    "    model_classifier.parameters(), args.optim, args.lr_C, args.clip, weight_decay = args.weight_decay,\n",
    ")\n",
    "        \n",
    "\n",
    "############################################\n",
    "####### generator/classifier training ######\n",
    "############################################\n",
    "classifier_trn_loss_old = 1000000\n",
    "tst_report_old = None\n",
    "\n",
    "for epoch in range(args.epochs_GANs):\n",
    "    epoch_start_time = time.time() \n",
    "    \n",
    "    for c_index in range(1):\n",
    "        ########## preparing input for generator ##########\n",
    "        trn_noise_gen = torch.randn((gen_num, args.generator_win, Data.m))\n",
    "        Data_trn_full_normal_gen = [trn_noise_gen, None, None]\n",
    "\n",
    "        ################ generator generating fake data for classifier ######################         \n",
    "        fake_data_all = model_generator(Data_trn_full_normal_gen, model_normal); \n",
    "        fake_data_X = fake_data_all[:, 0:args.classifier_win, :]  ### cut data\n",
    "        fake_data_Y = torch.zeros((fake_data_all.shape[0], args.classifier_win, args.classifier_prewin, Data.m))\n",
    "        for pre_win_i in range(args.classifier_prewin):\n",
    "            fake_data_Y[:, :, pre_win_i, :] = fake_data_all[:, (pre_win_i+1):(pre_win_i+1+args.classifier_win), :]\n",
    "        fake_data_label = torch.zeros((len(fake_data_X), len(data_paths)-1))\n",
    "        for fake_class_i in range(len(data_paths)-1):\n",
    "            fake_sampleidx_start = fake_class_i*gen_num\n",
    "            fake_sampleidx_end   = (fake_class_i+1)*gen_num\n",
    "            fake_data_label[fake_sampleidx_start:fake_sampleidx_end, fake_class_i] = 1\n",
    "\n",
    "        \n",
    "        ########### prepraing training input for classifier by merging real and fake data ############        \n",
    "        X_trn_merged      = torch.cat((X_trn_org[:, 0:args.classifier_win,:], fake_data_X), dim = 0)\n",
    "        Y_trn_full_merged = torch.cat((Y_trn_full_org[:, 0:args.classifier_win, :, :], fake_data_Y), dim = 0)\n",
    "        Label_merged = torch.zeros((len(X_trn_merged), 2*len(data_paths)-1))   ### label\n",
    "        Label_merged[0:len(Label_trn_org), 0:len(data_paths)] = Label_trn_org  \n",
    "        Label_merged[len(Label_trn_org):, len(data_paths):] = fake_data_label  \n",
    "        Data_trn_merge = [X_trn_merged, Y_trn_full_merged, Label_merged]   \n",
    "        ################## train and update classifier ###################\n",
    "        classifier_trn_loss, trn_report = train_classifier(Data, Data_trn_merge, model_classifier, criterion_2, optim_classifier, args.batch_size, model_normal)\n",
    "        trn_macro_f1 = trn_report['macro avg']['f1-score']   \n",
    "            \n",
    "    ###################### update generator #############   \n",
    "    for g_index in range(1):\n",
    "        \n",
    "        model_generator.train();\n",
    "        model_generator.zero_grad()\n",
    "        \n",
    "        trn_noise_gen = torch.randn((gen_num, args.generator_win, Data.m))\n",
    "        Data_trn_full_normal_gen = [trn_noise_gen, None, None]\n",
    "        fake_data_all = model_generator(Data_trn_full_normal_gen, model_normal); \n",
    "        fake_data_X = fake_data_all[:, 0:args.classifier_win, :]  ### cut data\n",
    "        fake_data_Y = torch.zeros((fake_data_all.shape[0], args.classifier_win, args.classifier_prewin, Data.m))\n",
    "        for pre_win_i in range(args.classifier_prewin):\n",
    "            fake_data_Y[:, :, pre_win_i, :] = fake_data_all[:, (pre_win_i+1):(pre_win_i+1+args.classifier_win), :]\n",
    "\n",
    "                \n",
    "        ################## get regression feedback for generator from classifier ##########\n",
    "        prediction_from_classifier = model_classifier([fake_data_X], model_normal)    \n",
    "        ################## get label loss of generator output and update generator ##########\n",
    "        fake_data_Y_duplicate = fake_data_Y.clone()\n",
    "        for class_idx in range(model_classifier.num_class-1):\n",
    "            fake_data_Y_duplicate = torch.cat((fake_data_Y_duplicate, fake_data_Y),dim=0)\n",
    "        fake_loss_all = criterion_2(prediction_from_classifier, fake_data_Y_duplicate); \n",
    "        \n",
    "        feedback_label_tmp = torch.sum(fake_loss_all, dim=1)\n",
    "        feedback_label_tmp = torch.sum(feedback_label_tmp, dim=1)\n",
    "        feedback_label_tmp = torch.sum(feedback_label_tmp, dim=1)\n",
    "        predict_label_soft = feedback_label_tmp.view(2*len(data_paths)-1, fake_data_X.shape[0]).transpose(0,1)            \n",
    "        predict_label_soft_invert = 1/(predict_label_soft)#torch.exp(1/(predict_label_soft+1))\n",
    "        predict_label_soft_norm = F.normalize(predict_label_soft_invert, p=1, dim=1)           \n",
    "        fake_labels_groundtruth = torch.zeros((predict_label_soft_norm.shape))\n",
    "        for class_idx in range(model_generator.num_class):\n",
    "            start_idx_label = (class_idx+1)*gen_num\n",
    "            end_idx_label = (class_idx+2)*gen_num\n",
    "            fake_labels_groundtruth[start_idx_label:end_idx_label, class_idx+1] = 1\n",
    "        fake_labels_groundtruth = torch.tensor(torch.argmax(fake_labels_groundtruth, 1), dtype=torch.long)\n",
    "        generator_loss = criterion_3(predict_label_soft_norm, fake_labels_groundtruth); \n",
    "        \n",
    "        graph_reg = 0\n",
    "        G_predict_gen, G_predict_gen_org = model_generator.predict_relationship(model_normal)     \n",
    "        num_graph = len(G_predict_gen)\n",
    "        for graph_i in range(num_graph-1):\n",
    "            for graph_j in range(graph_i+1, num_graph):\n",
    "                graph_A = G_predict_gen[graph_i].reshape(Data.m*Data.m); \n",
    "                graph_B = G_predict_gen[graph_j].reshape(Data.m*Data.m);\n",
    "                graph_reg = graph_reg + 1/LA.norm(graph_A-graph_B)      \n",
    "        generator_loss = generator_loss + 0.01*graph_reg \n",
    "        \n",
    "        generator_loss.backward(retain_graph=True)\n",
    "        optim_generator.step()  \n",
    "        generator_loss_sum = torch.sum(generator_loss)\n",
    "               \n",
    "    ##################### testing classifier ######################\n",
    "    classifier_tst_loss, tst_report, MF, GM, PC, AR = evaluate_classifier(Data, Data_tst_full, model_classifier, criterion_2, optim_classifier, args.batch_size)\n",
    "    tst_macro_f1 = tst_report['macro avg']['f1-score']       \n",
    "\n",
    "    classifier_trn_loss_old = classifier_trn_loss\n",
    "    tst_report_old = tst_report\n",
    "    print('GAN Rround {:3d}|T:{:5.2f}s|classifier trn loss {:5.4f}|generator loss {:5.4f}|testset MF1 {:5.4f}|'.format(epoch, (time.time() - epoch_start_time), classifier_trn_loss, generator_loss_sum, tst_macro_f1))\n",
    "    print('MF1: ',MF, ' GM: ',GM, ' PC: ',PC, ' AR: ',AR)\n",
    "  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
